"""
Run RAG pipeline evaluation on QA triplets.

This script:
1. Loads QA triplets generated by generate_questions.py
2. For each question, retrieves top-k passages and generates answer
3. Records retrieved passages, generated answer, and source passage rank
"""

import argparse
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

from tqdm import tqdm  # type: ignore

from dnd_dm_copilot.api.services.retriever import FAISSRetriever
from dnd_dm_copilot.model.lfm2_client import LFM2Client

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def load_qa_triplets(qa_file: str) -> List[Dict[str, Any]]:
    """
    Load QA triplets from JSON file.

    Args:
        qa_file: Path to QA triplets JSON file

    Returns:
        List of QA triplet dictionaries

    Raises:
        FileNotFoundError: If file doesn't exist
    """
    qa_path = Path(qa_file)
    if not qa_path.exists():
        raise FileNotFoundError(f"QA triplets file not found: {qa_file}")

    with open(qa_path, "r", encoding="utf-8") as f:
        qa_triplets: List[Dict[str, Any]] = json.load(f)

    logger.info(f"Loaded {len(qa_triplets)} QA triplets from {qa_file}")
    return qa_triplets


def find_passage_rank(
    source_passage: str, retrieved_passages: List[Dict[str, Any]]
) -> Optional[int]:
    """
    Find rank of source passage in retrieved results.

    Args:
        source_passage: The ground truth source passage text
        retrieved_passages: List of retrieved passage dicts with 'text' field

    Returns:
        Rank (0-indexed) of source passage, or None if not found
    """
    for rank, passage in enumerate(retrieved_passages):
        if passage["text"] == source_passage:
            return rank
    return None


def save_results(results: List[Dict[str, Any]], output_file: str) -> None:
    """
    Save pipeline results to JSON file.

    Args:
        results: List of result dictionaries
        output_file: Path to output JSON file
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    logger.info(f"Saved {len(results)} results to {output_file}")


class RAGPipelineRunner:
    """Runner for evaluating RAG pipeline on QA triplets."""

    def __init__(
        self,
        model_path: str,
        index_path: str,
        llm_model_path: str,
        top_k: int = 10,
    ):
        """
        Initialize RAG pipeline runner.

        Args:
            model_path: Path to sentence-transformer model
            index_path: Path to FAISS index directory
            llm_model_path: Path to LFM2 GGUF model file
            top_k: Number of passages to retrieve (0 disables RAG)
        """
        logger.info("Initializing RAG pipeline runner")

        self.top_k = top_k

        # Initialize retriever only if RAG is enabled
        if top_k > 0:
            self.retriever = FAISSRetriever(model_path=model_path)
            self.retriever.load_index(index_path)
            logger.info(f"RAG enabled with top_k={top_k}")
        else:
            self.retriever = None
            logger.info("RAG disabled (top_k=0) - baseline mode")

        # Initialize LLM client
        self.llm_client = LFM2Client(model_path=llm_model_path)

        logger.info("Pipeline initialized")

    def run_single_query(self, qa_triplet: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run RAG pipeline on a single QA triplet.

        Args:
            qa_triplet: Dict with 'question', 'answer', 'passage', 'metadata'

        Returns:
            Dict with:
                - question: Original question
                - ground_truth_answer: Expected answer
                - source_passage: Source passage text
                - retrieved_passages: List of retrieved passages with scores (empty if top_k=0)
                - source_passage_rank: Rank of source passage (or None)
                - generated_answer: LLM-generated answer
                - metadata: Original metadata
        """
        question = qa_triplet["question"]
        logger.debug(f"Running query: {question}")

        # Retrieve passages only if RAG is enabled
        if self.top_k > 0:
            retrieved_passages = self.retriever.search(  # type: ignore
                query=question, top_k=self.top_k
            )
            source_passage = qa_triplet["passage"]
            source_rank = find_passage_rank(source_passage, retrieved_passages)
            context_texts = [p["text"] for p in retrieved_passages]
        else:
            # Baseline mode: no retrieval, no context
            retrieved_passages = []
            source_rank = None
            context_texts = []

        # Generate answer (with or without context)
        generated_answer = self.llm_client.generate(
            query=question, context=context_texts, temperature=0.0, max_tokens=512
        )

        return {
            "question": question,
            "ground_truth_answer": qa_triplet["answer"],
            "source_passage": qa_triplet["passage"],
            "retrieved_passages": retrieved_passages,
            "source_passage_rank": source_rank,
            "generated_answer": generated_answer,
            "metadata": qa_triplet.get("metadata", {}),
        }

    def run_batch(
        self, qa_triplets: List[Dict[str, Any]], skip_errors: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Run RAG pipeline on batch of QA triplets.

        Args:
            qa_triplets: List of QA triplet dictionaries
            skip_errors: If True, continue on errors; if False, raise

        Returns:
            List of result dictionaries
        """
        results = []

        for qa_triplet in tqdm(qa_triplets, desc="Running RAG pipeline"):
            try:
                result = self.run_single_query(qa_triplet)
                results.append(result)
            except Exception as e:
                logger.error(f"Error processing query '{qa_triplet['question']}': {e}")
                if not skip_errors:
                    raise

        logger.info(f"Processed {len(results)} queries successfully")
        return results


def main() -> None:
    """Main entry point for RAG pipeline evaluation."""
    parser = argparse.ArgumentParser(
        description="Run RAG pipeline evaluation on QA triplets"
    )
    parser.add_argument(
        "--qa_triplets",
        type=str,
        required=True,
        help="Path to QA triplets JSON file",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default="models/sbert/",
        help="Path to sentence-transformer model",
    )
    parser.add_argument(
        "--index_path",
        type=str,
        default="data/indices/mechanics/",
        help="Path to FAISS index directory",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="data/evaluation/rag_results.json",
        help="Path to output results JSON file",
    )
    parser.add_argument(
        "--llm_model_path",
        type=str,
        required=True,
        help="Path to LFM2 GGUF model file",
    )
    parser.add_argument(
        "--top_k",
        type=int,
        default=10,
        help="Number of passages to retrieve (set to 0 to disable RAG for baseline)",
    )
    parser.add_argument(
        "--skip_errors",
        action="store_true",
        help="Continue processing on errors instead of stopping",
    )

    args = parser.parse_args()

    try:
        # Load QA triplets
        logger.info(f"Loading QA triplets from {args.qa_triplets}")
        qa_triplets = load_qa_triplets(args.qa_triplets)

        # Initialize pipeline
        runner = RAGPipelineRunner(
            model_path=args.model_path,
            index_path=args.index_path,
            llm_model_path=args.llm_model_path,
            top_k=args.top_k,
        )

        # Run pipeline
        logger.info("Running RAG pipeline...")
        results = runner.run_batch(qa_triplets, skip_errors=args.skip_errors)

        # Save results
        save_results(results, args.output)
        logger.info(f"Successfully processed {len(results)} queries")

    except Exception as e:
        logger.error(f"RAG pipeline evaluation failed: {e}")
        raise


if __name__ == "__main__":
    main()
